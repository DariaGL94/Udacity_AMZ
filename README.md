# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains personal data about bank customers that are part of a bankmarketing compain. We seek to predict whether the campaign will be successful or not for a customer.
**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model is a VotingEnsemble from AutoML with AUC weighted score of 0.94945.
## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The first step in the pipeline archtecture is the cleaning of the data. Therefore all categorical variables such as marital, housing, loan etc. are one hot encoded. NaNs are removed and dates set in the right format. Then the dataset is split into a training (70%) and test set (30%). We are using a logistic regression (binary classification of y) with hyperparameters C (inverse of regulation strength) and max_iter (maximum numbers of iterations to converge). Smaller values of C cause stronger regularization. After the hyperparameter tuning we get a logistic regression with parameters C=1.59629 and max_iter=80 with an accuracy of 0.91148.
**What are the benefits of the parameter sampler you chose?**
The Random sampling supports discrete (max_iter) as well as continuous (C) hyperparameters. Random sampling can be used as an initial search with an later refinement for better results. It also supports early termination, so it is a low-budget solution in contrast to Grid sampling or Bayesian sampling.
**What are the benefits of the early stopping policy you chose?**
The Bandit policy prevents that an algorithm is taking too many iterations to every posiible combination of hyperparameters without producing better results. It is based on factor/slack amount of the most successful job.
## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML results in an VotingEnsemble consisting of a SparseNormalizer.LightGBM, a StandardScalerWrapper.XGBoost, a MaxAbsScaler.LightGBM and a MaxAbsScaler.XGBoostClassifier. Each of this algorithms has a different ensemble weight and different hyperparameters. Comparing to a simple logistig regression, this algorithm is very complex.
## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The performance of the two models is quite similar. The VotingEnsemle has a slightly better accuracy than the logistic regression. It's a more complex model and consisting of an ensemble of different algorithms with different ensemble weights. A logistic regression is simplier and easier to understand for the user.
## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
The AutoML pipeline indicates a class balancing problem, so there might be a model bias. Imbalanced data can lead to a falsely perceived positive effect of a model's accuracy. To improve future experiments, this problem should be fixed in the training data.

